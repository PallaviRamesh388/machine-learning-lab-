{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'playtennis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-edb277be45f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;31m# load and prepare data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'data_banknote_authentication.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;31m# convert string attributes to integers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-edb277be45f2>\u001b[0m in \u001b[0;36mload_csv\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Load a CSV file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplaytennis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'playtennis' is not defined"
     ]
    }
   ],
   "source": [
    "# CART on the Bank Note dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    " \n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tfile = open(playtennis.csv, \"rb\")\n",
    "\tlines = reader(file)\n",
    "\tdataset = list(lines)\n",
    "\treturn dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    " \n",
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "\tleft, right = list(), list()\n",
    "\tfor row in dataset:\n",
    "\t\tif row[index] < value:\n",
    "\t\t\tleft.append(row)\n",
    "\t\telse:\n",
    "\t\t\tright.append(row)\n",
    "\treturn left, right\n",
    " \n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "\t# count all samples at split point\n",
    "\tn_instances = float(sum([len(group) for group in groups]))\n",
    "\t# sum weighted Gini index for each group\n",
    "\tgini = 0.0\n",
    "\tfor group in groups:\n",
    "\t\tsize = float(len(group))\n",
    "\t\t# avoid divide by zero\n",
    "\t\tif size == 0:\n",
    "\t\t\tcontinue\n",
    "\t\tscore = 0.0\n",
    "\t\t# score the group based on the score for each class\n",
    "\t\tfor class_val in classes:\n",
    "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
    "\t\t\tscore += p * p\n",
    "\t\t# weight the group score by its relative size\n",
    "\t\tgini += (1.0 - score) * (size / n_instances)\n",
    "\treturn gini\n",
    " \n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset):\n",
    "\tclass_values = list(set(row[-1] for row in dataset))\n",
    "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "\tfor index in range(len(dataset[0])-1):\n",
    "\t\tfor row in dataset:\n",
    "\t\t\tgroups = test_split(index, row[index], dataset)\n",
    "\t\t\tgini = gini_index(groups, class_values)\n",
    "\t\t\tif gini < b_score:\n",
    "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    " \n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "\toutcomes = [row[-1] for row in group]\n",
    "\treturn max(set(outcomes), key=outcomes.count)\n",
    " \n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth):\n",
    "\tleft, right = node['groups']\n",
    "\tdel(node['groups'])\n",
    "\t# check for a no split\n",
    "\tif not left or not right:\n",
    "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
    "\t\treturn\n",
    "\t# check for max depth\n",
    "\tif depth >= max_depth:\n",
    "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "\t\treturn\n",
    "\t# process left child\n",
    "\tif len(left) <= min_size:\n",
    "\t\tnode['left'] = to_terminal(left)\n",
    "\telse:\n",
    "\t\tnode['left'] = get_split(left)\n",
    "\t\tsplit(node['left'], max_depth, min_size, depth+1)\n",
    "\t# process right child\n",
    "\tif len(right) <= min_size:\n",
    "\t\tnode['right'] = to_terminal(right)\n",
    "\telse:\n",
    "\t\tnode['right'] = get_split(right)\n",
    "\t\tsplit(node['right'], max_depth, min_size, depth+1)\n",
    " \n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size):\n",
    "\troot = get_split(train)\n",
    "\tsplit(root, max_depth, min_size, 1)\n",
    "\treturn root\n",
    " \n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "\tif row[node['index']] < node['value']:\n",
    "\t\tif isinstance(node['left'], dict):\n",
    "\t\t\treturn predict(node['left'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['left']\n",
    "\telse:\n",
    "\t\tif isinstance(node['right'], dict):\n",
    "\t\t\treturn predict(node['right'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['right']\n",
    " \n",
    "# Classification and Regression Tree Algorithm\n",
    "def decision_tree(train, test, max_depth, min_size):\n",
    "\ttree = build_tree(train, max_depth, min_size)\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\tprediction = predict(tree, row)\n",
    "\t\tpredictions.append(prediction)\n",
    "\treturn(predictions)\n",
    " \n",
    "# Test CART on Bank Note dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'data_banknote_authentication.csv'\n",
    "dataset = load_csv(filename)\n",
    "# convert string attributes to integers\n",
    "for i in range(len(dataset[0])):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "max_depth = 5\n",
    "min_size = 10\n",
    "scores = evaluate_algorithm(dataset, decision_tree, n_folds, max_depth, min_size)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Gunjan\\\\Desktop\\\\ML_Dataset\\\\ID3\\\\PlayTennis.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a6bdb3d8cb04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;31m#''' Main program '''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\Gunjan\\\\Desktop\\\\ML_Dataset\\\\ID3\\\\PlayTennis.csv\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Read Tennis data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Build decision tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-a6bdb3d8cb04>\u001b[0m in \u001b[0;36mload_csv\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Gunjan\\\\Desktop\\\\ML_Dataset\\\\ID3\\\\PlayTennis.csv'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import csv\n",
    "\n",
    "def load_csv(filename):\n",
    "    lines = csv.reader(open(filename, \"r\"));\n",
    "    dataset = list(lines)\n",
    "    headers = dataset.pop(0)\n",
    "    return dataset, headers\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, attribute):\n",
    "        self.attribute = attribute\n",
    "        self.children = []\n",
    "        self.answer = \"\"  # NULL indicates children exists. # Not Null indicates this is a Leaf Node\n",
    "\n",
    "def subtables(data, col, delete): \n",
    "    dic = {}\n",
    "    coldata = [ row[col] for row in data]\n",
    "    attr = list(set(coldata)) # All values of attribute retrived\n",
    "    for k in attr:\n",
    "        dic[k] = []\n",
    "\n",
    "    for y in range(len(data)):\n",
    "        key = data[y][col]\n",
    "        if delete:\n",
    "            del data[y][col]\n",
    "        dic[key].append(data[y])\n",
    "    return attr, dic\n",
    "\n",
    "def entropy(S):\n",
    "    attr = list(set(S))\n",
    "    if len(attr) == 1: #if all are +ve/-ve then entropy = 0 \n",
    "        return 0\n",
    "\n",
    "    counts = [0,0] # Only two values possible 'yes' or 'no' \n",
    "    for i in range(2):\n",
    "        counts[i] = sum( [1 for x in S if attr[i] == x] ) / (len(S) * 1.0)\n",
    "\n",
    "    sums = 0\n",
    "    for cnt in counts:\n",
    "        sums += -1 * cnt * math.log(cnt, 2)\n",
    "    return sums\n",
    "\n",
    "def compute_gain(data, col):\n",
    "    attValues, dic = subtables(data, col, delete=False)\n",
    "    total_entropy = entropy([row[-1] for row in data])\n",
    "    for x in range(len(attValues)):\n",
    "        ratio = len(dic[attValues[x]]) / ( len(data) * 1.0)\n",
    "        entro = entropy([row[-1] for row in dic[attValues[x]]]) \n",
    "        total_entropy -= ratio*entro\n",
    "\n",
    "    return total_entropy\n",
    " \n",
    "def build_tree(data, features):\n",
    "    lastcol = [row[-1] for row in data]\n",
    "    if (len(set(lastcol))) == 1: # If all samples have same labels return that label\n",
    "        node=Node(\"\")\n",
    "        node.answer = lastcol[0]\n",
    "        return node\n",
    "\n",
    "    n = len(data[0])-1\n",
    "    gains = [compute_gain(data, col) for col in range(n) ]\n",
    "\n",
    "    split = gains.index(max(gains)) # Find max gains and returns index \n",
    "    node = Node(features[split]) # 'node' stores attribute selected #del (features[split])\n",
    "    fea = features[:split]+features[split+1:]\n",
    "\n",
    "    attr, dic = subtables(data, split, delete=True) # Data will be spilt in subtables \n",
    "    for x in range(len(attr)):\n",
    "        child = build_tree(dic[attr[x]], fea) \n",
    "        node.children.append((attr[x], child))\n",
    "\n",
    "    return node\n",
    "\n",
    "def print_tree(node, level):\n",
    "    if node.answer != \"\":\n",
    "        print(\"     \"*level, node.answer) # Displays leaf node yes/no \n",
    "        return\n",
    "\n",
    "    print(\"       \"*level, node.attribute) # Displays attribute Name \n",
    "    for value, n in node.children:\n",
    "        print(\"     \"*(level+1), value) \n",
    "        print_tree(n, level + 2)\n",
    "\n",
    "def classify(node,x_test,features): \n",
    "    if node.answer != \"\":\n",
    "        print(node.answer) \n",
    "        return\n",
    "\n",
    "    pos = features.index(node.attribute)\n",
    "    for value, n in node.children:\n",
    "        if x_test[pos]==value: \n",
    "            classify(n,x_test,features)\n",
    "\n",
    "#''' Main program '''\n",
    "dataset, features = load_csv(\"C:\\\\Users\\\\Gunjan\\\\Desktop\\\\ML_Dataset\\\\ID3\\\\PlayTennis.csv\") # Read Tennis data \n",
    "node = build_tree(dataset, features) # Build decision tree\n",
    "\n",
    "print(\"The decision tree for the dataset using ID3 algorithm is \") \n",
    "print_tree(node, 0)\n",
    "\n",
    "testdata, features = load_csv(\"C:\\\\Users\\\\Gunjan\\\\Desktop\\\\ML_Dataset\\\\ID3\\\\a.csv\") \n",
    "for xtest in testdata:\n",
    "    print(\"The test instance : \",xtest) \n",
    "    print(\"The predicted label : \", end=\"\") \n",
    "    classify(node,xtest,features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 113)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m113\u001b[0m\n\u001b[1;33m    print(\"\\n For training Example No:{0} the hypothesis is\".format(i),hypothesis) print(\"\\n The Maximally specific hypothesis for the training instance is \") print(hypothesis)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import math \n",
    " \n",
    "df = pd.read_csv('/Users/Chachu/Documents/Python Scripts/PlayTennis.csv') print(\"\\n Input Data Set is:\\n\", df) \n",
    " \n",
    "t = df.keys()[-1] \n",
    "print('Target Attribute is: ', t) \n",
    "# Get the attribute names from input dataset \n",
    "attribute_names = list(df.keys()) \n",
    "#Remove the target attribute from the attribute names list attribute_names.remove(t)  \n",
    "print('Predicting Attributes: ', attribute_names) #Function to calculate the entropy of collection S def entropy(probs):   \n",
    " \n",
    " \n",
    " \n",
    "MACHINE LEARNING LABORATORY \n",
    "[As per Choice Based Credit System (CBCS) scheme] \n",
    "(Effective from the academic year 2017 - 2018) SEMESTER – VII \n",
    "Subject Code \t17CSL76 \tIA Marks \t40 \n",
    "Number of Lecture Hours/Week \t01I + 02P \tExam Marks \t60 \n",
    "Total Number of Lecture Hours \t40 \tExam Hours \t03 \n",
    "CREDITS – 02 \n",
    "Description (If any): \n",
    "1.\tThe programs can be implemented in either JAVA or Python. \n",
    "2.\tFor Problems 1 to 6 and 10, programs are to be developed without using the built-in classes or APIs of Java/Python. \n",
    "3.\tData \tsets \tcan \tbe \ttaken \tfrom \tstandard \trepositories \n",
    "(https://archive.ics.uci.edu/ml/datasets.html) or constructed by the students. \n",
    "Lab Experiments: \n",
    "1. Implement and demonstratethe FIND-Salgorithm for finding the most specific hypothesis based on a given set of training data samples. Read the training data from a .CSV file. \n",
    "2. For a given set of training data examples stored in a .CSV file, implement and demonstrate the Candidate-Elimination algorithmto output a description of the set of all hypotheses consistent with the training examples. \n",
    "3. Write a program to demonstrate the working of the decision tree  based  ID3 algorithm. Use an appropriate data set for building the decision tree and apply this knowledge toclassify a new sample. \n",
    "4. Build an Artificial Neural Network by implementing the Backpropagation algorithm and test the same using appropriate data sets. \n",
    "5.  Write a program to implement the naïve Bayesian classifier for a sample training  data set stored as a .CSV file. Compute the accuracy of the classifier, considering few test data sets. \n",
    "6. Assuming a set of documents that need to be classified, use the naïve Bayesian Classifier model to perform this task. Built-in Java classes/API can be used to write the program. Calculate the accuracy, precision, and recall for your data set. \n",
    "7. Write a program to construct aBayesian network considering medical data. Use this model to demonstrate the diagnosis of heart patients using standard Heart Disease Data Set. You can use Java/Python ML library classes/API. \n",
    "8. Apply EM algorithm to cluster a set of data stored in a .CSV file. Use the same data set for clustering using k-Means algorithm. Compare the results of these two algorithms and comment on the quality of clustering. You can add Java/Python ML library classes/API in the program. \n",
    "9. Write a program to implement k-Nearest Neighbour algorithm to classify the iris  data set. Print both correct and wrong predictions. Java/Python ML library classes can be used for this problem. \n",
    "10. Implement the non-parametric Locally Weighted Regressionalgorithm in order to fit data points. Select appropriate data set for your experiment and draw graphs. \n",
    "Study Experiment / Project: \n",
    "NIL \n",
    "Course outcomes: The students should be able to: \n",
    "1. Understand the implementation procedures for the machine learning algorithms. \n",
    " \n",
    "2.\tDesign Java/Python programs for various Learning algorithms. \n",
    "3.\tApply appropriate data sets to the Machine Learning algorithms. \n",
    "4.\tIdentify and apply Machine Learning algorithms to solve real world problems. \n",
    "Conduction of Practical Examination: \n",
    "•\tAll laboratory experiments are to be included for practical examination. \n",
    "•\tStudents are allowed to pick one experiment from the lot. \n",
    "•\tStrictly follow the instructions as printed on the cover page of answer script \n",
    "•\tMarks distribution: Procedure + Conduction + Viva:15 + 70 +15 (100) \n",
    "Change of experiment is allowed only once and marks allotted to the procedure part to be made zero. \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "1.  LAB PROGRAM: 1 \n",
    " \n",
    "2.\tTITLE: FIND-S ALGORITHM \n",
    "3.\tAIM: \n",
    "• Implement and demonstrate the FIND-S algorithm for finding the most specific hypothesis based  on a given set of training data samples. Read the training data from a .CSV file. \n",
    " \n",
    "4.\tFind-S Algorithm \n",
    " \n",
    "1.\tInitialize h to the most specific hypothesis in H \n",
    "2.\tFor each positive training instance x \n",
    "For each attribute constraint a i in h : \n",
    "If the constraint a i in h is satisfied by x then do nothing \n",
    "Else replace a i in h by the next more general constraint that is satisfied by x \n",
    "3.\tOutput hypothesis h \n",
    " \n",
    "5.\tImplementation/ Program 1: \n",
    " \n",
    "import csv \n",
    "num_attribute=6 \n",
    "a=[] with open('enjoysport.csv', 'r') as csvfile: \n",
    "    reader=csv.reader(csvfile)     for row in reader:         a.append(row)         print(row) \n",
    "print(\"\\n The total number of training instances are : \",len(a)) num_attribute = len(a[0])-1 print(\"\\n The initial hypothesis is : \") hypothesis = ['0']*num_attribute print(hypothesis) for j in range(0,num_attribute): \n",
    "    hypothesis[j]=a[0][j] \n",
    "print(\"\\n Find-S: Finding maximally specific Hypothesis\\n\") for i in range(0,len(a)):     if a[i][num_attribute]=='Yes': \n",
    "        for j in range(0,num_attribute):             if a[i][j]!=hypothesis[j]:                  hypothesis[j]='?'             else: \n",
    "                hypothesis[j]=a[i][j] \n",
    "    print(\"\\n For training Example No:{0} the hypothesis is\".format(i),hypothesis) print(\"\\n The Maximally specific hypothesis for the training instance is \") print(hypothesis) \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "6. Result/Output: \n",
    " \n",
    "  \n",
    " \n",
    " \n",
    " \n",
    "Training Data Set : enjoysport.csv \n",
    " \n",
    " \n",
    "  \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "1.  LAB PROGRAM: 2 \n",
    " \n",
    "2.\tTITLE: Candidate-Elimination algorithm \n",
    "3.\tAIM: \n",
    "• For a given set of training data examples stored in a .CSV file, implement and demonstrate the Candidate-Elimination algorithm to output a description of the set of all hypotheses consistent with the training examples. \n",
    " \n",
    "4.\tCandidate-Elimination algorithm: \n",
    " \n",
    "Initialize G to the set of maximally general hypotheses in H \n",
    "1.\tInitialize S to the set of maximally specific hypotheses in H \n",
    "2.\tFor each training example d, do \n",
    "2.1.\tIf d is a positive example \n",
    "Remove from G any hypothesis inconsistent with d , For each hypothesis s in S that is not consistent with d , \n",
    "Remove s from S \n",
    "Add to S all minimal generalizations h of s such that h is consistent with d, and some member of G is more general than h \n",
    "Remove from S, hypothesis that is more general than another hypothesis in S \n",
    "2.2.\tIf d is a negative example \n",
    "Remove from S any hypothesis inconsistent with d For each hypothesis g in G that is not consistent with d \n",
    "     Remove g from G \n",
    "Add to G all minimal specializations h of g such that h is consistent with d, and     \n",
    " \t some member of S is more specific than h \n",
    "                        Remove from G any hypothesis that is less general than another hypothesis in G \n",
    " \n",
    "5. Implementation/Program2: \n",
    " \n",
    "import csv a=[] with open(\"enjoysport.csv\",\"r\") as csvfile: \n",
    "    fdata=csv.reader(csvfile)     for row in fdata: \n",
    "        a.append(row)         print(row) num_att=len(a[0])-1 S=['0']*num_att G=['?']*num_att print(S) print(G) temp=[] for i in range(0,num_att):     S[i]=a[0][i] \n",
    "print(\"................................................\") for i in range(0,len(a)): \n",
    "    if a[i][num_att]==\"Yes\":         for j in range(0,num_att):             if S[j]!=a[i][j]:                    S[j]='?' \n",
    "        for j in range(0,num_att):               for k in range(0,len(temp)):                     if temp[k][j]!=S[j] and temp[k][j]!='?': \n",
    "                        del temp[k]     if a[i][num_att]=='No':         for j in range(0,num_att):             if a[i][j]!=S[j] and S[j]!='?':                 G[j]=S[j]                 temp.append(G)                 G=['?']*num_att     print(S)     if len(temp)==0: \n",
    "        print(G)     else:  \n",
    "        print(temp) \n",
    "    print(\"......................................................................\") \n",
    " \n",
    "6. Result/Output: \n",
    " \n",
    "  \n",
    " \n",
    "Training Data Set : enjoysport.csv \n",
    " \n",
    "  \n",
    " \n",
    "1. Lab Program : 3 \n",
    " \n",
    "2.\tTITLE: ID3 ALGORITHM \n",
    "3.\tAIM: \n",
    "Write a program to demonstrate the working of the decision tree based ID3 algorithm. Use appropriate data set for building the decision tree and apply this knowledge to classify a new sample. \n",
    "4.\tID3 algorithm: \n",
    "Algorithm: ID3(Examples, TargetAttribute, Attributes) Input: \n",
    "Examples are the training examples. \n",
    "Targetattribute is the attribute whose value is to be predicted by the tree. \n",
    "Attributes is a list of other attributes that may be tested by the learned decision tree. \n",
    "Output: Returns a decision tree that correctly classiJies the given Examples Method: \n",
    "1.\tCreate a Root node for the tree \n",
    "2.\tIf all Examples are positive, Return the single-node tree Root, with label = + \n",
    "3.\tIf all Examples are negative, Return the single-node tree Root, with label = - \n",
    "4.\tIf Attributes is empty, \n",
    "Return the single-node tree Root, with label = most common value of TargetAttribute in Examples \n",
    "Else \n",
    "A ← the attribute from Attributes that best classifies Examples The decision attribute for Root ←A For each possible value, vi, of A, \n",
    "Add a new tree branch below Root, corresponding to the test A = vi Let \n",
    "Examplesvi be the subset of Examples that have value vi for A \n",
    "If Examplesvi is empty Then below this new branch add a leaf node with label = most common value of TargetAttribute in Examples \n",
    "Else below this new branch add the subtree ID3(Examplesvi, TargetAttribute, Attributes–{A}) End \n",
    "Return Root \n",
    " \n",
    "5.\tImplementation/Program: \n",
    "import pandas as pd \n",
    "import math \n",
    " \n",
    "df = pd.read_csv('/Users/Chachu/Documents/Python Scripts/PlayTennis.csv') print(\"\\n Input Data Set is:\\n\", df) \n",
    " \n",
    "t = df.keys()[-1] \n",
    "print('Target Attribute is: ', t) \n",
    "# Get the attribute names from input dataset \n",
    "attribute_names = list(df.keys()) \n",
    "#Remove the target attribute from the attribute names list attribute_names.remove(t)  \n",
    "print('Predicting Attributes: ', attribute_names) #Function to calculate the entropy of collection S def entropy(probs):   \n",
    "    return sum( [-prob*math.log(prob, 2) for prob in probs]) #Function to calulate the entropy of the given Data Sets/List with  \n",
    "#respect to target attributes def entropy_of_list(ls,value):   \n",
    "    from collections import Counter \n",
    "    cnt = Counter(x for x in ls)# Counter calculates the propotion of class     print('Target attribute class count(Yes/No)=',dict(cnt))     total_instances = len(ls)   \n",
    "    print(\"Total no of instances/records associated with {0} is: {1}\".format(value,total_instances ))     probs = [x / total_instances for x in cnt.values()]  # x means no of YES/NO     print(\"Probability of Class {0} is: {1:.4f}\".format(min(cnt),min(probs)))     print(\"Probability of Class {0} is: {1:.4f}\".format(max(cnt),max(probs)))     return entropy(probs) # Call Entropy  \n",
    " \n",
    "def information_gain(df, split_attribute, target_attribute,battr): \n",
    "    print(\"\\n\\n-----Information Gain Calculation of \",split_attribute, \" --------\")      df_split = df.groupby(split_attribute) # group the data based on attribute values     glist=[]     for gname,group in df_split: \n",
    "        print('Grouped Attribute Values \\n',group)         glist.append(gname)  \n",
    "     \n",
    "    glist.reverse() \n",
    "    nobs = len(df.index) * 1.0    \n",
    "    df_agg1=df_split.agg({target_attribute:lambda x:entropy_of_list(x, glist.pop())})     df_agg2=df_split.agg({target_attribute :lambda x:len(x)/nobs}) \n",
    "     \n",
    "    df_agg1.columns=['Entropy'] \n",
    "    df_agg2.columns=['Proportion'] \n",
    "     \n",
    "    # Calculate Information Gain:     new_entropy = sum( df_agg1['Entropy'] * df_agg2['Proportion'])     if battr !='S':         old_entropy = entropy_of_list(df[target_attribute],'S-'+df.iloc[0][df.columns.get_loc(battr)])     else: \n",
    "        old_entropy = entropy_of_list(df[target_attribute],battr)     return old_entropy - new_entropy \n",
    " \n",
    " \n",
    " \n",
    "def id3(df, target_attribute, attribute_names, default_class=None,default_attr='S'): \n",
    "     \n",
    "    from collections import Counter \n",
    "    cnt = Counter(x for x in df[target_attribute])# class of YES /NO \n",
    "     \n",
    "    ## First check: Is this split of the dataset homogeneous? \n",
    "    if len(cnt) == 1: \n",
    "        return next(iter(cnt))  # next input data set, or raises StopIteration when EOF is hit. \n",
    "     \n",
    "    ## Second check: Is this split of the dataset empty? if yes, return a default value     elif df.empty or (not attribute_names): \n",
    "        return default_class  # Return None for Empty Data Set \n",
    "     \n",
    "    ## Otherwise: This dataset is ready to be devied up! \n",
    "    else: \n",
    "        # Get Default Value for next recursive call of this function:         default_class = max(cnt.keys()) #No of YES and NO Class         # Compute the Information Gain of the attributes:         gainz=[]         for attr in attribute_names: \n",
    "            ig= information_gain(df, attr, target_attribute,default_attr)             gainz.append(ig) \n",
    "            print('Information gain of ',attr,' is : ',ig) \n",
    "         \n",
    "        index_of_max = gainz.index(max(gainz))                        best_attr = attribute_names[index_of_max         print(\"\\nAttribute with the maximum gain is: \", best_attr)         # Create an empty tree, to be populated in a moment         tree = {best_attr:{}} # Initiate the tree with best attribute as a node          remaining_attribute_names =[i for i in attribute_names if i != best_attr] \n",
    "         \n",
    "        # Split dataset-On each split, recursively call this algorithm.Populate the empty tree with subtrees, which \n",
    "        # are the result of the recursive call         for attr_val, data_subset in df.groupby(best_attr):             subtree = id3(data_subset,target_attribute, remaining_attribute_names,default_class,best_attr) \n",
    "            tree[best_attr][attr_val] = subtree         return tree \n",
    "     \n",
    "    from pprint import pprint tree = id3(df,t,attribute_names) print(\"\\nThe Resultant Decision Tree is:\") print(tree) \n",
    " \n",
    "def classify(instance, tree,default=None): # Instance of Play Tennis with Predicted         attribute = next(iter(tree)) # Outlook/Humidity/Wind        \n",
    "    if instance[attribute] in tree[attribute].keys(): # Value of the attributs in  set of Tree keys           result = tree[attribute][instance[attribute]] \n",
    "        if isinstance(result, dict): # this is a tree, delve deeper             return classify(instance, result)         else: \n",
    "            return result # this is a label     else: \n",
    "        return default \n",
    "     \n",
    "df_new=pd.read_csv('/Users/Chachu/Documents/Python Scripts/PlayTennisTest.csv') df_new['predicted'] = df_new.apply(classify, axis=1, args=(tree,'?'))  print(df_new) \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \t \n",
    "6. Result/Output: \n",
    " \n",
    "  \n",
    " \n",
    " \n",
    "Training Data Set : PlayTennis.csv \n",
    " \n",
    "  \n",
    " \n",
    "Testing Data Set : PlayTennisTest.csv \n",
    " \n",
    "  \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "11. LAB PROGRAM: 4 \n",
    "2.\tTITLE: BACKPROPAGATION ALGORITHM \n",
    "3.\tAIM: \n",
    "    Build an Artificial Neural Network by implementing the Back propagation algorithm and test the  same using appropriate data sets. \n",
    " \n",
    "4.\tBackpropagation Algorithm: \n",
    " \n",
    "  \n",
    "5.\tImplementation/ Program:  \n",
    " \n",
    "import numpy as np \n",
    "X = np.array(([2, 9], [1, 5], [3, 6]))  y = np.array(([92], [86], [89]))  \n",
    "y = y/100  \n",
    " \n",
    "def sigmoid(x):  \n",
    "    return 1/(1 + np.exp(-x)) \n",
    " \n",
    "def derivatives_sigmoid(x): \n",
    "    return x * (1 - x) \n",
    " \n",
    "epoch=10000  lr=0.1  \n",
    "inputlayer_neurons = 2  hiddenlayer_neurons = 3  \n",
    "output_neurons = 1  \n",
    " \n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons)) bias_hidden=np.random.uniform(size=(1,hiddenlayer_neurons))  \n",
    "weight_hidden=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))  bias_output=np.random.uniform(size=(1,output_neurons))  \n",
    " \n",
    "for i in range(epoch):     hinp1=np.dot(X,wh)     hinp= hinp1 + bias_hidden  \n",
    "    hlayer_activation = sigmoid(hinp) \n",
    "     \n",
    "    outinp1=np.dot(hlayer_activation,weight_hidden)     outinp= outinp1+ bias_output \n",
    "    output = sigmoid(outinp) \n",
    "      \n",
    "    EO = y-output  \n",
    "    outgrad = derivatives_sigmoid(output)      d_output = EO * outgrad  \n",
    "    EH = d_output.dot(weight_hidden.T)   \n",
    "    hiddengrad = derivatives_sigmoid(hlayer_activation)      d_hiddenlayer = EH * hiddengrad \n",
    " \n",
    "    weight_hidden += hlayer_activation.T.dot(d_output) *lr \n",
    "    bias_hidden += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr \n",
    " \n",
    "    wh += X.T.dot(d_hiddenlayer) *lr \n",
    "    bias_output += np.sum(d_output, axis=0,keepdims=True) *lr \n",
    " \n",
    "print(\"Input: \\n\" + str(X)) print(\"Actual Output: \\n\" + str(y)) \n",
    "print(\"Predicted Output: \\n\" ,output) \n",
    " \n",
    "6. Result/Output: \n",
    " \n",
    "  \n",
    " \n",
    " \n",
    "1. LAB PROGRAM: 5 \n",
    " \n",
    "2.\tTITLE: NAÏVE BAYESIAN CLASSIFIER \n",
    "3.\tAIM: \n",
    "Write a program to implement the naïve Bayesian classifier for a sample training data set stored as a .CSV file. Compute the accuracy of the classifier, considering few test data sets. \n",
    " \n",
    "4.\tAlgorithm: \n",
    "   \n",
    "5.\tImplementation/Program : \n",
    "import numpy as np import math import csv import pdb def read_data(filename): \n",
    " \n",
    "    with open(filename,'r') as csvfile:         datareader = csv.reader(csvfile)         metadata = next(datareader) \n",
    "        traindata=[]         for row in datareader:             traindata.append(row) \n",
    " \n",
    "    return (metadata, traindata) \n",
    " \n",
    "def splitDataset(dataset, splitRatio): \n",
    "    trainSize = int(len(dataset) * splitRatio)     trainSet = []     testset = list(dataset) \n",
    "    i=0     while len(trainSet) < trainSize:         trainSet.append(testset.pop(i))     return [trainSet, testset] \n",
    " \n",
    "def classify(data,test): \n",
    " \n",
    "    total_size = data.shape[0] \n",
    "    print(\"\\n\") \n",
    "    print(\"training data size=\",total_size) \n",
    "    print(\"test data size=\",test.shape[0]) \n",
    " \n",
    "    countYes = 0     countNo = 0     probYes = 0     probNo = 0     print(\"\\n\") \n",
    "    print(\"target    count    probability\") \n",
    " \n",
    "    for x in range(data.shape[0]): \n",
    "        if data[x,data.shape[1]-1] == 'yes': \n",
    "            countYes +=1         if data[x,data.shape[1]-1] == 'no': \n",
    "            countNo +=1 \n",
    " \n",
    "    probYes=countYes/total_size \n",
    "    probNo= countNo / total_size \n",
    " \n",
    "    print('Yes',\"\\t\",countYes,\"\\t\",probYes) \n",
    "    print('No',\"\\t\",countNo,\"\\t\",probNo) \n",
    " \n",
    " \n",
    "    prob0 =np.zeros((test.shape[1]-1))     prob1 =np.zeros((test.shape[1]-1))     accuracy=0     print(\"\\n\") \n",
    "    print(\"instance prediction  target\") \n",
    " \n",
    "    for t in range(test.shape[0]):         for k in range (test.shape[1]-1): \n",
    "            count1=count0=0             for j in range (data.shape[0]):                 #how many times appeared with no                 if test[t,k] == data[j,k] and data[j,data.shape[1]-1]=='no': \n",
    "                    count0+=1 \n",
    "                #how many times appeared with yes                 if test[t,k]==data[j,k] and data[j,data.shape[1]-1]=='yes': \n",
    "                    count1+=1             prob0[k]=count0/countNo \n",
    "            prob1[k]=count1/countYes \n",
    " \n",
    "        probno=probNo         probyes=probYes         for i in range(test.shape[1]-1):             probno=probno*prob0[i]             probyes=probyes*prob1[i]         if probno>probyes: \n",
    "            predict='no'         else: \n",
    "            predict='yes' \n",
    " \n",
    "        print(t+1,\"\\t\",predict,\"\\t    \",test[t,test.shape[1]-1])         if predict == test[t,test.shape[1]-1]: \n",
    "            accuracy+=1     final_accuracy=(accuracy/test.shape[0])*100     print(\"accuracy\",final_accuracy,\"%\")     return \n",
    " \n",
    "metadata,traindata= read_data(\"/Users/Chachu/Documents/Python Scripts/tennis.csv\") splitRatio=0.6 \n",
    "trainingset, testset=splitDataset(traindata, splitRatio) training=np.array(trainingset) print(\"\\n The Training data set are:\") for x in trainingset: \n",
    "    print(x) \n",
    "     \n",
    "testing=np.array(testset) print(\"\\n The Test data set are:\") for x in testing: \n",
    "    print(x) \n",
    "classify(training,testing) \n",
    " \n",
    " \n",
    " \n",
    "6. Result /Output: \n",
    "                           \n",
    " \n",
    " \n",
    " \n",
    "Training Data Set : tennis.csv \n",
    " \n",
    "  \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "1. LAB PROGRAM: 6 \n",
    " \n",
    "2.\tTITLE: DOCUMENT CLASSIFICATION USING NAÏVE BAYESIAN CLASSIFIER \n",
    "3.\tAIM: \n",
    "• \tAssuming a set of documents that need to be classified, use the naïve Bayesian Classifier model to perform this task. Built-in Java classes/API can be used to write the program. \n",
    "Calculate the accuracy, precision, and recall for your data set. \n",
    "4.\tAlgorithm: \n",
    "  \n",
    " \n",
    " \n",
    "Analysis of Document Classification: \n",
    " \n",
    "  \n",
    "•\tFor classification tasks, the terms true positives, true negatives, false positives, and false negatives compare the results of the classifier under test with trusted external judgments. The terms positive and negative refer to the classifier's prediction (sometimes known as the expectation), and the terms true and false refer to whether that prediction corresponds to the external judgment (sometimes known as the observation). \n",
    "•\tPrecision - Precision is the ratio of correctly predicted positive documents to the total predicted positive documents. High precision relates to the low false positive rate. \n",
    "Precision = (Σ True positive ) / ( Σ True positive + Σ False positive) \n",
    "•\tRecall (Sensitivity) - Recall is the ratio of correctly predicted positive documents to the all observations in actual class. \n",
    "Recall = (Σ True positive ) / ( Σ True positive + Σ False negative) \n",
    "•\tAccuracy - Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model. For our model, we have got 0.803 which means our model is approx. 80% accurate. \n",
    "                                                     Accuracy = (Σ True positive + Σ True negative) / Σ Total population \n",
    " \n",
    "5.\tImplementation/ Program: \n",
    " \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn import metrics \n",
    " \n",
    "msg=pd.read_csv('/Users/Chachu/Documents/PythonScripts/naivetext.csv',names=['message',' label']) \n",
    " \n",
    "print('The dimensions of the dataset',msg.shape) \n",
    " \n",
    "msg['labelnum']=msg.label.map({'pos':1,'neg':0}) X=msg.message \n",
    "y=msg.labelnum \n",
    " \n",
    "#splitting the dataset into train and test data xtrain,xtest,ytrain,ytest=train_test_split(X,y) print ('\\n the total number of Training Data :',ytrain.shape) print ('\\n the total number of Test Data :',ytest.shape) \n",
    " \n",
    "#output the words or Tokens in the text documents cv = CountVectorizer() xtrain_dtm = cv.fit_transform(xtrain) xtest_dtm=cv.transform(xtest) print('\\n The words or Tokens in the text documents \\n') print(cv.get_feature_names()) df=pd.DataFrame(xtrain_dtm.toarray(),columns=cv.get_feature_names()) # Training Naive Bayes (NB) classifier on training data. clf = MultinomialNB().fit(xtrain_dtm,ytrain) predicted = clf.predict(xtest_dtm) \n",
    "#printing accuracy, Confusion matrix, Precision and Recall \n",
    "print('\\n Accuracy of the classifier is',metrics.accuracy_score(ytest,predicted)) print('\\n Confusion matrix') \n",
    "print(metrics.confusion_matrix(ytest,predicted)) \n",
    "print('\\n The value of Precision', metrics.precision_score(ytest,predicted)) print('\\n The value of Recall', metrics.recall_score(ytest,predicted)) \n",
    " \n",
    " \n",
    " \n",
    "6. Result/ Output: \n",
    " \n",
    "  \n",
    " \n",
    "Training Data Set : naivetext.csv \n",
    " \n",
    "  \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "1. LAB PROGRAM: 7 \n",
    " \n",
    "2.\tTITLE: BAYESIAN NETWORK \n",
    "3.\tAIM: \n",
    "Write a program to construct a Bayesian network considering medical data. Use this model to demonstrate the diagnosis of heart patients using standard Heart Disease Data Set. You can use Java/Python ML library classes/API. \n",
    "4.\tHeart Disease Data set \n",
    "  \n",
    "5.\tImplementation/ Program 7: \n",
    "import numpy as np import pandas as pd import csv from pgmpy.estimators import MaximumLikelihoodEstimator from pgmpy.models import BayesianModel from pgmpy.inference import VariableElimination \n",
    "#read Cleveland Heart Disease data heartDisease = pd.read_csv('/Users/Chachu/Documents/Python Scripts/heart.csv') heartDisease = heartDisease.replace('?',np.nan) \n",
    "#display the data print('Sample instances from the dataset are given below') print(heartDisease.head()) \n",
    "#display the Attributes names and datatyes print('\\n Attributes and datatypes') print(heartDisease.dtypes) #Creat Model- Bayesian Network model \n",
    "=BayesianModel([('age','heartdisease'),('sex','heartdisease'),('exang','heartdisease'),('cp','heartdisease'),(' heartdisease', \n",
    "'restecg'),('heartdisease','chol')]) \n",
    "#Learning CPDs using Maximum Likelihood Estimators print('\\n Learning CPD using Maximum likelihood estimators') model.fit(heartDisease,estimator=MaximumLikelihoodEstimator) \n",
    "# Inferencing with Bayesian Network \n",
    "print('\\n Inferencing with Bayesian Network:') \n",
    "HeartDiseasetest_infer = VariableElimination(model) #computing the Probability of HeartDisease given restecg print('\\n 1.Probability of HeartDisease given evidence=restecg :1') q1=HeartDiseasetest_infer.query(variables=['heartdisease'],evidence={'restecg':1}) print(q1) \n",
    "#computing the Probability of HeartDisease given cp print('\\n 2.Probability of HeartDisease given evidence= cp:2 ') q2=HeartDiseasetest_infer.query(variables=['heartdisease'],evidence={'cp':2}) print(q2) \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "6. Result/Output: \n",
    " \n",
    "   \n",
    "Training Data Set : heart.csv (Sample) \n",
    "  \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "1. LAB PROGRAM: 8 \n",
    " \n",
    "2.\tTITLE: CLUSTERING BASED ON EM ALGORITHM AND K-MEANS \n",
    "3.\tAIM:     \n",
    "  Apply EM algorithm to cluster a set of data stored in a .CSV file. Use the same data set for clustering using k-Means algorithm. Compare the results of these two algorithms and comment on the quality of clustering. You can add Java/Python ML library classes/API in the program. \n",
    " \n",
    "4.\tTHEORY: \n",
    "Expectation Maximization algorithm \n",
    "•\tThe basic approach and logic of this clustering method is as follows. \n",
    "•\tSuppose we measure a single continuous variable in a large sample of observations. Further, suppose that the sample consists of two clusters of observations with different means (and perhaps different standard deviations); within each sample, the distribution of values for the continuous variable follows the normal distribution. \n",
    "•\tThe goal of EM clustering is to estimate the means and standard deviations for each cluster so as to maximize the likelihood of the observed data (distribution). \n",
    "•\tPut another way, the EM algorithm attempts to approximate the observed distributions of values based on mixtures of different distributions in different clusters. The results of EM clustering are different from those computed by k-means clustering. \n",
    "•\tThe latter will assign observations to clusters to maximize the distances between clusters. The EM algorithm does not compute actual assignments of observations to clusters, but classification probabilities. \n",
    "•\tIn other words, each observation belongs to each cluster with a certain probability. Of course, as a final result we can usually review an actual assignment of observations to clusters, based on the (largest) classification probability. K means Clustering \n",
    "•\tThe algorithm will categorize the items into k groups of similarity. To calculate that similarity, we will use the euclidean distance as measurement. \n",
    "•\tThe algorithm works as follows: \n",
    "1.\tFirst we initialize k points, called means, randomly. \n",
    "2.\tWe categorize each item to its closest mean and we update the mean’s coordinates, which are the averages of the items categorized in that mean so far. \n",
    "3.\tWe repeat the process for a given number of iterations and at the end, we have our clusters. \n",
    "•\tThe “points” mentioned above are called means, because they hold the mean values of the items categorized in it. To initialize these means, we have a lot of options. An intuitive method is to initialize the means at random items in the data set. Another method is to initialize the means at random values between the boundaries of the data set (if for a feature x the items have values in [0,3], we will initialize the means with values for x at [0,3]). \n",
    "•\tPseudocode: \n",
    "1.\tInitialize k means with random values \n",
    "2.\tFor a given number of iterations: Iterate through items: \n",
    "Find the mean closest to the item Assign item to mean \n",
    " \t \t                    Update mean \n",
    " \n",
    " \n",
    "5.\tImplementation/Program : \n",
    " \n",
    "import matplotlib.pyplot as plt  from sklearn import datasets from sklearn.cluster import KMeans  import pandas as pd \n",
    "import numpy as np  \n",
    "                                         # import some data to play with  iris = datasets.load_iris() \n",
    "X = pd.DataFrame(iris.data) \n",
    "X.columns =  ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']  y = pd.DataFrame(iris.target) y.columns = ['Targets'] \n",
    " \n",
    "# Build the K Means Model model = KMeans(n_clusters=3) \n",
    "model.fit(X)        # model.labels_ : Gives cluster no for which samples belongs to \n",
    " \n",
    "# # Visualise the clustering results  plt.figure(figsize=(14,14)) colormap = np.array(['red', 'lime', 'black']) \n",
    "# Plot the Original Classifications using Petal features  plt.subplot(2, 2, 1) \n",
    "plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[y.Targets], s=40)  plt.title('Real Clusters') plt.xlabel('Petal Length')  plt.ylabel('Petal Width') \n",
    "# Plot the Models Classifications \n",
    "plt.subplot(2, 2, 2) \n",
    "plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[model.labels_], s=40)  plt.title('K-Means Clustering') plt.xlabel('Petal Length')  plt.ylabel('Petal Width') \n",
    " \n",
    "# General EM for GMM \n",
    "from sklearn import preprocessing \n",
    "# transform your data such that its distribution will have a # mean value 0 and standard deviation of 1. scaler = preprocessing.StandardScaler()  scaler.fit(X) \n",
    "xsa = scaler.transform(X) \n",
    "xs = pd.DataFrame(xsa, columns = X.columns) \n",
    " \n",
    "from sklearn.mixture import GaussianMixture  gmm = GaussianMixture(n_components=3)  gmm.fit(xs) \n",
    "gmm_y = gmm.predict(xs) plt.subplot(2, 2, 3) plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[gmm_y], s=40) plt.title('GMM Clustering') plt.xlabel('Petal Length')  plt.ylabel('Petal Width') \n",
    "print('Observation: The GMM using EM algorithm based clustering matched the true labels \n",
    "more closely than the Kmeans.') \n",
    " \n",
    " \n",
    "    6. Result/Output: \n",
    " \n",
    "  \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "1. LAB PROGRAM: 9 \n",
    " \n",
    "2.\tTITLE: K-NEAREST NEIGHBOUR \n",
    "3.\tAIM:     \n",
    "  Write a program to implement k-Nearest Neighbour algorithm to classify the iris data set. Print both correct and wrong predictions. Java/Python ML library classes can be used for this problem. \n",
    "4.\tTHEORY: \n",
    "•\tK-Nearest Neighbors is one of the most basic yet essential classification algorithms in Machine Learning. It belongs to the supervised learning domain and finds intense application in pattern recognition, data mining and intrusion detection. \n",
    "•\tIt is widely disposable in real-life scenarios since it is non-parametric, meaning, it does not make any underlying assumptions about the distribution of data. \n",
    " \n",
    "•\tAlgorithm \n",
    "Input: Let m be the number of training data samples. Let p be an unknown point. \n",
    "Method: \n",
    "1.\tStore the training samples in an array of data points arr[]. This means each element of this array represents a tuple (x, y). \n",
    "2.\tfor i=0 to m \n",
    "Calculate Euclidean distance d(arr[i], p). \n",
    "3.\tMake set S of K smallest distances obtained. Each of these distances correspond to an already classified data point. \n",
    "                          Return the majority label among S. \n",
    " \n",
    "5.\tImplementation/ Program: \n",
    "from sklearn.model_selection import train_test_split  from sklearn.neighbors import KNeighborsClassifier  from sklearn import datasets \n",
    " \n",
    "# Load dataset  iris=datasets.load_iris()  \n",
    "print(\"Iris Data set loaded...\") \n",
    " \n",
    "# Split the data into train and test samples x_train, x_test, y_train, y_test = train_test_split(iris.data,iris.target,test_size=0.1)  print(\"Dataset is split into training and testing...\") \n",
    "print(\"Size of trainng data and its label\",x_train.shape,y_train.shape)  print(\"Size of trainng data and its label\",x_test.shape, y_test.shape) \n",
    " \n",
    "# Prints Label no. and their names  for i in range(len(iris.target_names)):     print(\"Label\", i , \"-\",str(iris.target_names[i])) \n",
    "    # Create object of KNN classifier \n",
    "classifier = KNeighborsClassifier(n_neighbors=1) \n",
    " \n",
    " \n",
    "# Perform Training  \n",
    "classifier.fit(x_train, y_train) # Perform testing y_pred=classifier.predict(x_test) \n",
    " \n",
    "# Display the results \n",
    "print(\"Results of Classification using K-nn with K=1 \")  for r in range(0,len(x_test)): \n",
    "    print(\" Sample:\", str(x_test[r]), \" Actual-label:\", str(y_test[r]), \" Predicted-label:\", str(y_pred[r])) print(\"Classification Accuracy :\" , classifier.score(x_test,y_test)); \n",
    " \n",
    "from sklearn.metrics import classification_report, confusion_matrix print('Confusion Matrix') \n",
    "print(confusion_matrix(y_test,y_pred))  print('Accuracy Metrics')  \n",
    "print(classification_report(y_test,y_pred)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m52\u001b[0m\n\u001b[1;33m    index_of_max = gainz.index(max(gainz))                        best_attr = attribute_names[index_of_max         print(\"\\nAttribute with the maximum gain is: \", best_attr)         # Create an empty tree, to be populated in a moment         tree = {best_attr:{}} # Initiate the tree with best attribute as a node          remaining_attribute_names =[i for i in attribute_names if i != best_attr]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import math \n",
    " \n",
    "df = pd.read_csv('/Users/Chachu/Documents/Python Scripts/playTennis.csv') print(\"\\n Input Data Set is:\\n\", df) \n",
    " \n",
    "t = df.keys()[-1] \n",
    "print('Target Attribute is: ', t) \n",
    "# Get the attribute names from input dataset \n",
    "attribute_names = list(df.keys()) \n",
    "#Remove the target attribute from the attribute names list attribute_names.remove(t)  \n",
    "print('Predicting Attributes: ', attribute_names) #Function to calculate the entropy of collection S def entropy(probs):   \n",
    "    return sum( [-prob*math.log(prob, 2) for prob in probs]) #Function to calulate the entropy of the given Data Sets/List with  \n",
    "#respect to target attributes def entropy_of_list(ls,value):   \n",
    "    from collections import Counter \n",
    "    cnt = Counter(x for x in ls)# Counter calculates the propotion of class     print('Target attribute class count(Yes/No)=',dict(cnt))     total_instances = len(ls)   \n",
    "    print(\"Total no of instances/records associated with {0} is: {1}\".format(value,total_instances ))     probs = [x / total_instances for x in cnt.values()]  # x means no of YES/NO     print(\"Probability of Class {0} is: {1:.4f}\".format(min(cnt),min(probs)))     print(\"Probability of Class {0} is: {1:.4f}\".format(max(cnt),max(probs)))     return entropy(probs) # Call Entropy  \n",
    " \n",
    "def information_gain(df, split_attribute, target_attribute,battr): \n",
    "    print(\"\\n\\n-----Information Gain Calculation of \",split_attribute, \" --------\")      df_split = df.groupby(split_attribute) # group the data based on attribute values     glist=[]     for gname,group in df_split: \n",
    "        print('Grouped Attribute Values \\n',group)         glist.append(gname)  \n",
    "     \n",
    "    glist.reverse() \n",
    "    nobs = len(df.index) * 1.0    \n",
    "    df_agg1=df_split.agg({target_attribute:lambda x:entropy_of_list(x, glist.pop())})     df_agg2=df_split.agg({target_attribute :lambda x:len(x)/nobs}) \n",
    "     \n",
    "    df_agg1.columns=['Entropy'] \n",
    "    df_agg2.columns=['Proportion'] \n",
    "     \n",
    "    # Calculate Information Gain:     new_entropy = sum( df_agg1['Entropy'] * df_agg2['Proportion'])     if battr !='S':         old_entropy = entropy_of_list(df[target_attribute],'S-'+df.iloc[0][df.columns.get_loc(battr)])     else: \n",
    "        old_entropy = entropy_of_list(df[target_attribute],battr)     return old_entropy - new_entropy \n",
    " \n",
    " \n",
    " \n",
    "def id3(df, target_attribute, attribute_names, default_class=None,default_attr='S'): \n",
    "     \n",
    "    from collections import Counter \n",
    "    cnt = Counter(x for x in df[target_attribute])# class of YES /NO \n",
    "     \n",
    "    ## First check: Is this split of the dataset homogeneous? \n",
    "    if len(cnt) == 1: \n",
    "        return next(iter(cnt))  # next input data set, or raises StopIteration when EOF is hit. \n",
    "     \n",
    "    ## Second check: Is this split of the dataset empty? if yes, return a default value     elif df.empty or (not attribute_names): \n",
    "        return default_class  # Return None for Empty Data Set \n",
    "     \n",
    "    ## Otherwise: This dataset is ready to be devied up! \n",
    "    else: \n",
    "        # Get Default Value for next recursive call of this function:         default_class = max(cnt.keys()) #No of YES and NO Class         # Compute the Information Gain of the attributes:         gainz=[]         for attr in attribute_names: \n",
    "            ig= information_gain(df, attr, target_attribute,default_attr)             gainz.append(ig) \n",
    "            print('Information gain of ',attr,' is : ',ig) \n",
    "         \n",
    "        index_of_max = gainz.index(max(gainz))                        best_attr = attribute_names[index_of_max         print(\"\\nAttribute with the maximum gain is: \", best_attr)         # Create an empty tree, to be populated in a moment         tree = {best_attr:{}} # Initiate the tree with best attribute as a node          remaining_attribute_names =[i for i in attribute_names if i != best_attr] \n",
    "         \n",
    "        # Split dataset-On each split, recursively call this algorithm.Populate the empty tree with subtrees, which \n",
    "        # are the result of the recursive call         for attr_val, data_subset in df.groupby(best_attr):             subtree = id3(data_subset,target_attribute, remaining_attribute_names,default_class,best_attr) \n",
    "            tree[best_attr][attr_val] = subtree         return tree \n",
    "     \n",
    "    from pprint import pprint tree = id3(df,t,attribute_names) print(\"\\nThe Resultant Decision Tree is:\") print(tree) \n",
    " \n",
    "def classify(instance, tree,default=None): # Instance of Play Tennis with Predicted         attribute = next(iter(tree)) # Outlook/Humidity/Wind        \n",
    "    if instance[attribute] in tree[attribute].keys(): # Value of the attributs in  set of Tree keys           result = tree[attribute][instance[attribute]] \n",
    "        if isinstance(result, dict): # this is a tree, delve deeper             return classify(instance, result)         else: \n",
    "            return result # this is a label     else: \n",
    "        return default \n",
    "     \n",
    "df_new=pd.read_csv('/Users/Chachu/Documents/Python Scripts/PlayTennisTest.csv') df_new['predicted'] = df_new.apply(classify, axis=1, args=(tree,'?'))  print(df_new) \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-63e9c3b01ae8>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-63e9c3b01ae8>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    X = np.array(([2, 9], [1, 5], [3, 6]))  Y = np.array(([92], [86], [89]))\u001b[0m\n\u001b[1;37m                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "X = np.array(([2, 9], [1, 5], [3, 6]))  Y = np.array(([92], [86], [89]))  \n",
    "y = y/100  \n",
    " \n",
    "def sigmoid(x):  \n",
    "    return 1/(1 + np.exp(-x)) \n",
    " \n",
    "def derivatives_sigmoid(x): \n",
    "    return x * (1 - x) \n",
    " \n",
    "epoch=10000  lr=0.1  \n",
    "inputlayer_neurons = 2  hiddenlayer_neurons = 3  \n",
    "output_neurons = 1  \n",
    " \n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons)) bias_hidden=np.random.uniform(size=(1,hiddenlayer_neurons))  \n",
    "weight_hidden=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))  bias_output=np.random.uniform(size=(1,output_neurons))  \n",
    " \n",
    "for i in range(epoch):     hinp1=np.dot(X,wh)     hinp= hinp1 + bias_hidden  \n",
    "    hlayer_activation = sigmoid(hinp) \n",
    "     \n",
    "    outinp1=np.dot(hlayer_activation,weight_hidden)     outinp= outinp1+ bias_output \n",
    "    output = sigmoid(outinp) \n",
    "      \n",
    "    EO = y-output  \n",
    "    outgrad = derivatives_sigmoid(output)      d_output = EO * outgrad  \n",
    "    EH = d_output.dot(weight_hidden.T)   \n",
    "    hiddengrad = derivatives_sigmoid(hlayer_activation)      d_hiddenlayer = EH * hiddengrad \n",
    " \n",
    "    weight_hidden += hlayer_activation.T.dot(d_output) *lr \n",
    "    bias_hidden += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr \n",
    " \n",
    "    wh += X.T.dot(d_hiddenlayer) *lr \n",
    "    bias_output += np.sum(d_output, axis=0,keepdims=True) *lr \n",
    " \n",
    "print(\"Input: \\n\" + str(X)) print(\"Actual Output: \\n\" + str(y)) \n",
    "print(\"Predicted Output: \\n\" ,output) \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Input: \n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.89461888]\n",
      " [0.87922034]\n",
      " [0.89597774]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # numpy is commonly used to process number array\n",
    "\n",
    "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float) # Features ( Hrs Slept, Hrs Studied) \n",
    "y = np.array(([92], [86], [89]), dtype=float)\t# Labels(Marks obtained)\n",
    "\n",
    "X = X/np.amax(X,axis=0) # Normalize \n",
    "y = y/100\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "def sigmoid_grad(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Variable initialization\n",
    "epoch=1000\t#Setting training iterations\n",
    "eta =0.2\t\t#Setting learning rate (eta)\n",
    "input_neurons = 2\t#number of features in data set \n",
    "hidden_neurons = 3\t#number of hidden layers neurons\n",
    "output_neurons = 1\t#number of neurons at output layer\n",
    "\n",
    "# Weight and bias - Random initialization\n",
    "wh=np.random.uniform(size=(input_neurons,hidden_neurons))\t# 2x3 \n",
    "bh=np.random.uniform(size=(1,hidden_neurons))\t# 1x3 \n",
    "wout=np.random.uniform(size=(hidden_neurons,output_neurons)) # 1x1 \n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "for i in range(epoch):\n",
    "#Forward Propogation\n",
    "    h_ip=np.dot(X,wh) + bh\t# Dot product + bias \n",
    "    h_act = sigmoid(h_ip)\t# Activation function \n",
    "    o_ip=np.dot(h_act,wout) + bout\n",
    "    output = sigmoid(o_ip)\n",
    "\n",
    "#Backpropagation\n",
    "    # Error at Output layer\n",
    "    Eo = y-output\t# Error at o/p \n",
    "    outgrad = sigmoid_grad(output)\n",
    "    d_output = Eo* outgrad\t# Errj=Oj(1-Oj)(Tj-Oj)\n",
    "\n",
    "    # Error at Hidden later\n",
    "    Eh = d_output.dot(wout.T)\t# .T means transpose\n",
    "    hiddengrad = sigmoid_grad(h_act)\t# How much hidden layer wts contributed to error \n",
    "    d_hidden = Eh * hiddengrad\n",
    "    wout += h_act.T.dot(d_output) *eta\t# Dotproduct of nextlayererror and currentlayerop \n",
    "    wh += X.T.dot(d_hidden) *eta\n",
    "\n",
    "print(\"Normalized Input: \\n\" + str(X))\n",
    "print(\"Actual Output: \\n\" + str(y))\n",
    "print(\"Predicted Output: \\n\" ,output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
